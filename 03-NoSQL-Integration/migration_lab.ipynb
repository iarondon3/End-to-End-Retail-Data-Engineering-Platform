{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKQEFQkgOXXXwfmurrl6aN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iarondon3/End-to-End-Retail-Data-Ecosystem/blob/main/03-NoSQL-Integration/migration_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üçÉ‚öôÔ∏è Step 1: Install & Start MongoDB *(Click Play)*"
      ],
      "metadata": {
        "id": "llMfKwULRa57"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OHm2Q0JRQzK"
      },
      "outputs": [],
      "source": [
        "# STRATEGY CHANGE: Using 'mongomock' to bypass Colab infrastructure limitations.\n",
        "# This simulates a perfect MongoDB instance in RAM for ETL demonstration.\n",
        "\n",
        "import time\n",
        "import os\n",
        "\n",
        "start_time = time.time()\n",
        "print(\"üì¶ Installing Python drivers...\")\n",
        "\n",
        "# Install mongomock alongside standard libraries\n",
        "os.system(\"pip install mongomock pymongo faker pandas > /dev/null 2>&1\")\n",
        "\n",
        "print(\"üöÄ Initializing In-Memory MongoDB Engine...\")\n",
        "# We don't need to start a Linux service. The database lives in Python now.\n",
        "from mongomock import MongoClient\n",
        "\n",
        "# Verify we can instantiate a client\n",
        "try:\n",
        "    client = MongoClient()\n",
        "    db = client.test_db\n",
        "    print(f\"‚úÖ In-Memory NoSQL Engine Ready!\")\n",
        "    print(\"   State: Active (RAM-based)\")\n",
        "    print(\"   Compatibility: Full PyMongo API support\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Setup Failed: {e}\")\n",
        "\n",
        "elapsed = round(time.time() - start_time, 2)\n",
        "print(f\"‚è±Ô∏è Setup completed in {elapsed} seconds.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **üîç What just happened?**\n",
        "\n",
        "> We initialized an **In-Memory NoSQL Engine** using `mongomock`.\n",
        "> * **Infrastructure strategy:** In professional CI/CD pipelines and ephemeral environments (like Colab), creating a heavy database service can be flaky. By using an in-memory instance, we guarantee 100% reliability for this demo.\n",
        "> * **Compatibility:** This engine accepts standard MongoDB commands (PyMongo), allowing us to test our data modeling logic."
      ],
      "metadata": {
        "id": "vd1zCk7HdBOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèóÔ∏è Step 2: Generate \"Relational\" Source Data (SQL Simulation)\n"
      ],
      "metadata": {
        "id": "Gkh-DhILR1Mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from faker import Faker\n",
        "import random\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# @markdown **Scenario Configuration: | Define Dataset Volume**\n",
        "SALES_VOLUME = 8000  # @param {type:\"slider\", min:1000, max:50000, step:1000}\n",
        "\n",
        "print(f\"üé≤ Generating synthetic SQL-like data ({SALES_VOLUME} transactions)...\")\n",
        "fake = Faker('en_US')\n",
        "\n",
        "# Quantities (Fixed Dimensions, Dynamic Sales)\n",
        "QUANTITIES = {\n",
        "    'branches': 50,      # Scaled down slightly for Colab RAM (originally 500)\n",
        "    'employees': 500,    # Scaled down (originally 5000)\n",
        "    'categories': 30,\n",
        "    'products': 2000,    # Scaled down (originally 8000)\n",
        "    'customers': 5000,   # Scaled down (originally 20000)\n",
        "    'sales': SALES_VOLUME # <--- LINKED TO THE SLIDER (Step 1)\n",
        "}\n",
        "\n",
        "WALGREENS_STATES = ['FL', 'TX', 'CA', 'IL', 'NY', 'PA', 'NC', 'GA', 'OH', 'MI']\n",
        "\n",
        "CATEGORIES_LIST = [\n",
        "    \"Prescription Drugs (Rx)\", \"Over-the-Counter (OTC)\", \"Pain Relief\", \"Vitamins & Supplements\",\n",
        "    \"Digestive Health\", \"Allergy & Sinus\", \"Wound Care\", \"First Aid\",\n",
        "    \"Skin Care\", \"Hair Care\", \"Oral Hygiene\", \"Feminine Care\",\n",
        "    \"Deodorants\", \"Makeup\", \"Facial Care\", \"Fragrances\", \"Baby Formula & Food\",\n",
        "    \"Diapers & Wipes\", \"Baby Care\", \"Household Cleaning\", \"Paper & Plastic\",\n",
        "    \"Batteries & Bulbs\", \"Snacks\", \"Beverages\", \"Candy\", \"Frozen Food\",\n",
        "    \"Breakfast & Cereal\", \"Photo & Electronics\", \"Contact Lenses\", \"Cards & Gifts\"\n",
        "]\n",
        "\n",
        "PRODUCT_TEMPLATES = {\n",
        "    \"Prescription Drugs (Rx)\": [\"Metformin 500mg\", \"Lisinopril 10mg\", \"Atorvastatin 20mg\"],\n",
        "    \"Over-the-Counter (OTC)\": [\"Allegra 24hr\", \"Zyrtec 10mg\", \"Claritin 24hr\", \"Pepto-Bismol\"],\n",
        "    \"Pain Relief\": [\"Ibuprofen 200mg\", \"Advil Liqui-Gels\", \"Tylenol Extra Strength\"],\n",
        "    \"Vitamins & Supplements\": [\"Vitamin C 1000mg\", \"Vitamin D3 2000IU\", \"Omega-3 Fish Oil\"],\n",
        "    \"Snacks\": [\"Lay's Classic Chips\", \"Doritos\", \"Cheetos\", \"Oreo Cookies\"],\n",
        "    \"Beverages\": [\"Coca-Cola 2L\", \"Pepsi 2L\", \"Bottled Water 1L\", \"Gatorade\"],\n",
        "    \"Household Cleaning\": [\"Clorox Wipes\", \"Lysol Spray\", \"Tide Detergent\"],\n",
        "    \"Electronics\": [\"USB-C Cable\", \"Anker Charger\", \"Apple EarPods\"]\n",
        "    # ... (Shortened for brevity in demo, but logic allows expansion)\n",
        "}\n",
        "\n",
        "print(f\"üé≤ Generating synthetic SQL-like data ({SALES_VOLUME} transactions)...\")\n",
        "fake = Faker('en_US')\n",
        "\n",
        "# --- 2. GENERATING DIMENSIONS (Flat Tables) ---\n",
        "\n",
        "# A. Branches\n",
        "print(f\"   ... Generating {QUANTITIES['branches']} Branches...\")\n",
        "branches = []\n",
        "for i in range(1, QUANTITIES['branches'] + 1):\n",
        "    branches.append({\n",
        "        \"branch_id\": i,\n",
        "        \"state\": random.choice(WALGREENS_STATES),\n",
        "        \"city\": fake.city(),\n",
        "        \"country\": \"USA\"\n",
        "    })\n",
        "\n",
        "# B. Customers\n",
        "print(f\"   ... Generating {QUANTITIES['customers']} Customers...\")\n",
        "customers = []\n",
        "for i in range(1, QUANTITIES['customers'] + 1):\n",
        "    customers.append({\n",
        "        \"customer_id\": i,\n",
        "        \"first_name\": fake.first_name(),\n",
        "        \"last_name\": fake.last_name(),\n",
        "        \"is_member\": random.choice([True, False]),\n",
        "        \"state\": random.choice(WALGREENS_STATES) # For geographic realism\n",
        "    })\n",
        "\n",
        "# C. Products (Complex Logic from SQL Script)\n",
        "print(f\"   ... Generating {QUANTITIES['products']} Products...\")\n",
        "products = []\n",
        "for i in range(1, QUANTITIES['products'] + 1):\n",
        "    cat_name = random.choice(CATEGORIES_LIST)\n",
        "\n",
        "    # Template logic\n",
        "    templates = PRODUCT_TEMPLATES.get(cat_name, [f\"Generic {cat_name} Product\"])\n",
        "    prod_name = random.choice(templates)\n",
        "    brand = random.choice([\"Walgreens\", \"Nice!\", \"Tylenol\", \"Dove\", \"Coca-Cola\", \"Generic\"])\n",
        "\n",
        "    products.append({\n",
        "        \"product_id\": i,\n",
        "        \"name\": prod_name,\n",
        "        \"brand\": brand,\n",
        "        \"category\": cat_name,\n",
        "        \"price\": round(random.uniform(1.99, 89.99), 2)\n",
        "    })\n",
        "\n",
        "# --- 3. GENERATING TRANSACTIONS (Simulating Relational Structure) ---\n",
        "# We simulate two tables: 'Sale' (Header) and 'Sale_Detail' (Line Items)\n",
        "\n",
        "print(f\"   ... Generating {QUANTITIES['sales']} Sales (Header & Details)...\")\n",
        "sql_sales = []        # Table: Sale\n",
        "sql_sale_details = [] # Table: Sale_Detail\n",
        "\n",
        "sale_id_counter = 1\n",
        "# Pre-fetch lists for performance\n",
        "branch_list = branches\n",
        "cust_list = customers\n",
        "prod_list = products\n",
        "\n",
        "# Weights for realistic distribution (Long Tail)\n",
        "# Most sales come from a few popular branches/customers\n",
        "branch_weights = [random.expovariate(1.5) for _ in branch_list]\n",
        "cust_weights = [random.expovariate(1.5) for _ in cust_list]\n",
        "\n",
        "for _ in range(QUANTITIES['sales']):\n",
        "    # 1. Foreign Keys (Simulated Joins)\n",
        "    # We use random.choices with weights for realism, or simple random for speed\n",
        "    branch = random.choices(branch_list, k=1)[0]\n",
        "    cust = random.choices(cust_list, k=1)[0]\n",
        "\n",
        "    # 2. Sale Header\n",
        "    sale_date = fake.date_this_year()\n",
        "    sql_sales.append({\n",
        "        \"sale_id\": sale_id_counter,\n",
        "        \"branch_id\": branch[\"branch_id\"],\n",
        "        \"customer_id\": cust[\"customer_id\"],\n",
        "        \"date\": str(sale_date),\n",
        "        \"channel\": random.choice([\"STORE\", \"ONLINE\", \"APP\"])\n",
        "    })\n",
        "\n",
        "    # 3. Sale Details (1 to 6 items per sale)\n",
        "    num_items = random.randint(1, 6)\n",
        "    selected_prods = random.sample(prod_list, num_items)\n",
        "\n",
        "    for prod in selected_prods:\n",
        "        qty = random.randint(1, 3)\n",
        "        sql_sale_details.append({\n",
        "            \"sale_id\": sale_id_counter, # FK to Header\n",
        "            \"product_id\": prod[\"product_id\"],\n",
        "            \"quantity\": qty,\n",
        "            \"unit_price\": prod[\"price\"]\n",
        "        })\n",
        "\n",
        "    sale_id_counter += 1\n",
        "\n",
        "# --- 4. VERIFICATION ---\n",
        "print(f\"‚úÖ Data Generation Complete!\")\n",
        "print(f\"   Simulated SQL Tables in Memory:\")\n",
        "print(f\"   - Branches: {len(branches)}\")\n",
        "print(f\"   - Products: {len(products)}\")\n",
        "print(f\"   - Customers: {len(customers)}\")\n",
        "print(f\"   - Sales (Header): {len(sql_sales)}\")\n",
        "print(f\"   - Sale Details (Lines): {len(sql_sale_details)}\")\n",
        "print(\"\\nüëâ Preview of 'Sale' Table (First 3 rows):\")\n",
        "print(pd.DataFrame(sql_sales[:3]))"
      ],
      "metadata": {
        "id": "jZ4IsyIFR10H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **üîç What just happened?**\n",
        "\n",
        "> We simulated a **Legacy Relational Database (SQL)** environment.\n",
        "> * **The Source:** We generated flat lists (`sql_sales`, `sql_sale_details`, `branches`) that mimic normalized SQL tables.\n",
        "> * **The Bottleneck:** Notice that the data is fragmented. To answer a simple question like *\"What did John buy?\"*, the system currently needs to perform expensive **JOINs** across 4 different tables. This is the friction point we aim to solve with NoSQL."
      ],
      "metadata": {
        "id": "wteNofJedTqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üèóÔ∏è Step 3: ETL Pipeline (SQL -> NoSQL Transformation)"
      ],
      "metadata": {
        "id": "nMaAMfVKaX77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime\n",
        "# We use the In-Memory client we set up in Step 1\n",
        "from mongomock import MongoClient\n",
        "\n",
        "print(\"üîÑ Starting ETL Process...\")\n",
        "start_etl = time.time()\n",
        "\n",
        "# --- 1. PRE-PROCESSING (Memory Indexing) ---\n",
        "# Simulating SQL Lookup Tables\n",
        "branch_map = {b['branch_id']: b for b in branches}\n",
        "product_map = {p['product_id']: p for p in products}\n",
        "customer_map = {c['customer_id']: c for c in customers}\n",
        "\n",
        "# Index sale details by sale_id for faster grouping\n",
        "details_map = {}\n",
        "for d in sql_sale_details:\n",
        "    s_id = d['sale_id']\n",
        "    if s_id not in details_map:\n",
        "        details_map[s_id] = []\n",
        "    details_map[s_id].append(d)\n",
        "\n",
        "print(f\"   ‚úì Dimensions indexed in memory.\")\n",
        "\n",
        "# --- 2. TRANSFORMATION (SQL Rows -> Nested Documents) ---\n",
        "mongo_docs = []\n",
        "\n",
        "for sale in sql_sales:\n",
        "    s_id = sale['sale_id']\n",
        "    b_id = sale['branch_id']\n",
        "    c_id = sale['customer_id']\n",
        "\n",
        "    # Retrieve Context (\"JOINs\")\n",
        "    branch_data = branch_map.get(b_id)\n",
        "    cust_data = customer_map.get(c_id)\n",
        "    sale_items = details_map.get(s_id, [])\n",
        "\n",
        "    # === DATA MODELING: PURE EMBEDDING ===\n",
        "    doc = {\n",
        "        \"sale_id\": s_id,\n",
        "        \"date\": datetime.strptime(sale['date'], \"%Y-%m-%d\"),\n",
        "        \"channel\": sale['channel'],\n",
        "\n",
        "        # Denormalized Customer\n",
        "        \"customer\": {\n",
        "            \"customer_id\": c_id,\n",
        "            \"first_name\": cust_data['first_name'],\n",
        "            \"last_name\": cust_data['last_name'],\n",
        "            \"is_member\": cust_data['is_member']\n",
        "        },\n",
        "\n",
        "        # Denormalized Branch\n",
        "        \"branch\": {\n",
        "            \"city\": branch_data['city'],\n",
        "            \"state\": branch_data['state'],\n",
        "            \"country\": \"USA\"\n",
        "        },\n",
        "\n",
        "        # Embedded Items Array\n",
        "        \"items\": []\n",
        "    }\n",
        "\n",
        "    # Transform Items\n",
        "    total_amount = 0\n",
        "    for item in sale_items:\n",
        "        prod = product_map.get(item['product_id'])\n",
        "        line_total = item['quantity'] * item['unit_price']\n",
        "        total_amount += line_total\n",
        "\n",
        "        doc[\"items\"].append({\n",
        "            \"product_name\": prod['name'],\n",
        "            \"brand\": prod['brand'],\n",
        "            \"category\": prod['category'],\n",
        "            \"quantity\": item['quantity'],\n",
        "            \"unit_price\": item['unit_price'],\n",
        "            \"line_total\": round(line_total, 2)\n",
        "        })\n",
        "\n",
        "    doc[\"total_amount\"] = round(total_amount, 2)\n",
        "    mongo_docs.append(doc)\n",
        "\n",
        "print(f\"   ‚úì Transformation complete. Prepared {len(mongo_docs)} nested documents.\")\n",
        "\n",
        "# --- 3. LOADING (Insert into NoSQL) ---\n",
        "# Connect to In-Memory DB\n",
        "client = MongoClient()\n",
        "db = client.walgreens_analytics\n",
        "collection = db.sales\n",
        "\n",
        "# Clear and Bulk Insert\n",
        "collection.delete_many({})\n",
        "collection.insert_many(mongo_docs)\n",
        "\n",
        "elapsed = round(time.time() - start_etl, 2)\n",
        "print(f\"‚úÖ ETL Finished in {elapsed}s. Data is loaded into MongoDB (Memory)!\")\n",
        "print(f\"   Collection Size: {collection.count_documents({})} documents.\")\n",
        "\n",
        "# --- 4. QUALITY ASSURANCE (Verify Schema) ---\n",
        "print(\"\\nüîé QA Check: Sampling one document to verify 'Pure Embedding' Schema:\")\n",
        "sample_doc = collection.find_one()\n",
        "# We use 'default=str' to handle datetime objects nicely in JSON\n",
        "print(json.dumps(sample_doc, indent=4, default=str))"
      ],
      "metadata": {
        "id": "k3heUvsXaYUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **üîç What just happened?**\n",
        "\n",
        "> We executed the core **ETL (Extract, Transform, Load)** process to migrate from SQL to NoSQL.\n",
        "> * **Transformation Pattern:** We applied the **Pure Embedding Strategy**. Instead of maintaining foreign keys, we rewrote the document by embedding the Customer, Branch, and Items directly into the Sale object.\n",
        "> * **The Result:** As seen in the QA Output, we now have **Rich Documents**. A single read operation retrieves the entire context of a transaction, optimizing \"Read-Heavy\" analytical workloads."
      ],
      "metadata": {
        "id": "KzaiApCYdmrD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Step 4: Run Analytics (Average Basket Size üõí)\n",
        "\n",
        "Calculates consumer behavior by analyzing embedded arrays."
      ],
      "metadata": {
        "id": "BnY9QZmIbaEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "\n",
        "print(\"üîé Executing Aggregation Pipeline: 'Average Basket Size per Customer'...\")\n",
        "\n",
        "pipeline = [\n",
        "    # 1. DOCUMENT LEVEL: Calculate total items in this specific transaction\n",
        "    # We use $addFields to create a temporary field 'basket_size' by summing the embedded array\n",
        "    {\n",
        "        \"$addFields\": {\n",
        "            \"basket_size\": { \"$sum\": \"$items.quantity\" }\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # 2. COLLECTION LEVEL: Group by Customer\n",
        "    {\n",
        "        \"$group\": {\n",
        "            \"_id\": {\n",
        "                \"first_name\": \"$customer.first_name\",\n",
        "                \"last_name\": \"$customer.last_name\"\n",
        "            },\n",
        "            \"avg_items_per_visit\": { \"$avg\": \"$basket_size\" },\n",
        "            \"total_visits\": { \"$sum\": 1 }\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # 3. SORT: Show customers with the largest baskets first\n",
        "    { \"$sort\": { \"avg_items_per_visit\": -1 } },\n",
        "\n",
        "    # 4. LIMIT: Top 10 for display\n",
        "    { \"$limit\": 10 }\n",
        "]\n",
        "\n",
        "# Execute\n",
        "results = list(collection.aggregate(pipeline))\n",
        "\n",
        "# Visualization\n",
        "print(f\"\\n{'CUSTOMER':<30} | {'AVG ITEMS/VISIT':<20} | {'TOTAL VISITS'}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for r in results:\n",
        "    full_name = f\"{r['_id']['first_name']} {r['_id']['last_name']}\"\n",
        "    avg = round(r['avg_items_per_visit'], 1)\n",
        "    visits = r['total_visits']\n",
        "    print(f\"{full_name:<30} | {avg:<20} | {visits}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Analysis Complete. This metric helps identify bulk buyers vs. impulsive shoppers.\")"
      ],
      "metadata": {
        "id": "is7RgHpbbZq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **üîç What just happened?**\n",
        "\n",
        "> We utilized the **MongoDB Aggregation Framework** to perform server-side analytics.\n",
        "> * **The Challenge:** We needed to calculate a metric (Basket Size) that didn't exist explicitly in the database.\n",
        "> * **The Solution:** We used a multi-stage pipeline. First, we used `$addFields` to sum the quantities inside the embedded `items` array (Document manipulation), and then we used `$group` to calculate the average across all purchases (Collection aggregation). This demonstrates MongoDB's capability to handle complex math beyond simple storage."
      ],
      "metadata": {
        "id": "Io1h1v97eC9l"
      }
    }
  ]
}