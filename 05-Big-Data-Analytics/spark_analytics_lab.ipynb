{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTUyySP9gIEXUXkLD3PIIC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iarondon3/End-to-End-Retail-Data-Ecosystem/blob/main/05-Big-Data-Analytics/spark_analytics_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚öôÔ∏è Step 1: Initialize Spark Environment\n",
        "\n"
      ],
      "metadata": {
        "id": "S3d5gRKg7v8a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzOI5d5V7ofq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "print(\"üì¶ Installing Dependencies (Java 8 + FindSpark)...\")\n",
        "\n",
        "# 1. Install Java 8 (The most stable version for Spark)\n",
        "# We use 'apt-get' to force this specific version\n",
        "os.system(\"apt-get update > /dev/null\")\n",
        "os.system(\"apt-get install openjdk-8-jdk-headless -qq > /dev/null\")\n",
        "\n",
        "# 2. Install Python Libraries\n",
        "os.system(\"pip install pyspark findspark plotly faker > /dev/null 2>&1\")\n",
        "\n",
        "# 3. Set Environment Variables\n",
        "# Force system to use Java 8\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# 4. Initialize Spark using FindSpark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "print(\"üöÄ Starting Spark Session...\")\n",
        "from pyspark.sql import SparkSession\n",
        "import plotly.express as px\n",
        "\n",
        "try:\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"Walgreens_BigData_Lab\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.ui.port\", \"4050\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    elapsed = round(time.time() - start_time, 2)\n",
        "    print(f\"‚úÖ Spark Session Created successfully in {elapsed}s.\")\n",
        "    print(f\"   Spark Version: {spark.version}\")\n",
        "    print(f\"   Java Used: {os.environ['JAVA_HOME']}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **üîç What just happened?**\n",
        "\n",
        "> We successfully set up a **Local Spark Environment** to simulate a professional Big Data platform like Databricks.\n",
        "> * **Dependencies:** We installed **Java 8** and **PySpark**, which are the essential requirements to run Apache Spark.\n",
        "> * **Engine Start:** We initialized the `SparkSession`. We are now ready to load and process massive datasets in the next steps."
      ],
      "metadata": {
        "id": "qY4EFE4WATmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  üèóÔ∏è Step 2: Generate Clickstream Data (Raw Logs)\n",
        "\n",
        "Simulating high-velocity web logs: Views, Cart Adds, and Payments."
      ],
      "metadata": {
        "id": "rzf_W_PV-cZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @markdown **Log Volume:**\n",
        "LOG_ENTRIES = 90000 # @param {type:\"slider\", min:10000, max:100000, step:10000}\n",
        "\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "fake = Faker()\n",
        "print(f\"üé≤ Generating {LOG_ENTRIES} raw web events...\")\n",
        "\n",
        "# --- WALGREENS PRODUCTS (Consistent) ---\n",
        "PRODUCTS = [\n",
        "    \"Metformin 500mg\", \"Lisinopril 10mg\", \"Atorvastatin 20mg\",\n",
        "    \"Allegra 24hr\", \"Zyrtec 10mg\", \"Claritin 24hr\",\n",
        "    \"CeraVe Moisturizing Cream\", \"Neutrogena Hydro Boost\",\n",
        "    \"Tylenol Extra Strength\", \"Advil Liqui-Gels\"\n",
        "]\n",
        "\n",
        "events_data = []\n",
        "# Simulation Logic:\n",
        "# Users generate sessions. Most view, some add to cart, few purchase.\n",
        "session_ids = [fake.uuid4() for _ in range(int(LOG_ENTRIES / 5))]\n",
        "\n",
        "for _ in range(LOG_ENTRIES):\n",
        "    session = random.choice(session_ids)\n",
        "\n",
        "    # Weighted Randomness to simulate a \"Leaky Funnel\"\n",
        "    # Notice: High probability of ERROR_PAYMENT vs PURCHASE_COMPLETE\n",
        "    event_type = random.choices(\n",
        "        ['VIEW_PRODUCT', 'ADD_TO_CART', 'CHECKOUT_START', 'PURCHASE_COMPLETE', 'ERROR_PAYMENT'],\n",
        "        weights=[0.50, 0.25, 0.15, 0.03, 0.07], # <--- 70% failure rate at checkout step simulated here\n",
        "        k=1\n",
        "    )[0]\n",
        "\n",
        "    events_data.append({\n",
        "        \"event_id\": fake.uuid4(),\n",
        "        \"timestamp\": fake.date_time_between(start_date='-1M', end_date='now').isoformat(),\n",
        "        \"session_id\": session,\n",
        "        \"user_id\": random.randint(1000, 5000), # Registered users\n",
        "        \"event_type\": event_type,\n",
        "        \"product_name\": random.choice(PRODUCTS),\n",
        "        \"device\": random.choice(['Mobile App', 'Desktop Web', 'Mobile Web']),\n",
        "        \"metadata\": {\"os\": random.choice([\"iOS\", \"Android\", \"Windows\"]), \"browser\": \"Chrome\"}\n",
        "    })\n",
        "\n",
        "# Create Spark DataFrame\n",
        "# In real life, we would read from S3/Data Lake: spark.read.json(\"s3://logs/...\")\n",
        "df_raw = spark.createDataFrame(events_data)\n",
        "\n",
        "print(f\"‚úÖ Data Generation Complete. Raw DataFrame Schema:\")\n",
        "df_raw.printSchema()\n",
        "print(\"   Sample Raw Event:\")\n",
        "df_raw.show(1, truncate=False)"
      ],
      "metadata": {
        "id": "z1vrNtSr-ct2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **üîç What just happened?**\n",
        "\n",
        "> We generated a **Raw Clickstream Dataset** representing user interactions on the Walgreens website.\n",
        "> * **Semi-Structured Data:** Unlike the clean tables in previous units, this data mimics JSON logs (common in Data Lakes), containing nested metadata and varying event types.\n",
        "> * **The Scenario:** We simulated a \"Leaky Funnel\" where many users reach the checkout page but fail to complete the purchase due to technical errors."
      ],
      "metadata": {
        "id": "1kE5PVpN-y8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîÑ Step 3: PySpark Transformation & Analysis\n",
        "\n",
        "Finding insights in massive logs using Distributed Computing logic."
      ],
      "metadata": {
        "id": "lg5e3KRQ-66l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "print(\"üõ†Ô∏è Executing Spark Transformations...\")\n",
        "\n",
        "# 1. FUNNEL ANALYSIS (Aggregation)\n",
        "# We group by event type to see the drop-off\n",
        "df_funnel = df_raw.groupBy(\"event_type\").count().orderBy(\"count\", ascending=False)\n",
        "\n",
        "# 2. PAYMENT FAILURE DETECTION (Filter)\n",
        "# Identifying technical issues at the gateway\n",
        "df_failures = df_raw.filter(F.col(\"event_type\") == \"ERROR_PAYMENT\")\n",
        "failure_count = df_failures.count()\n",
        "total_checkout_attempts = df_raw.filter(F.col(\"event_type\").isin([\"PURCHASE_COMPLETE\", \"ERROR_PAYMENT\"])).count()\n",
        "fail_rate = round((failure_count / total_checkout_attempts) * 100, 1)\n",
        "\n",
        "# 3. HIGH INTENT / NO PURCHASE (Anti-Join Strategy)\n",
        "# Find users who added to cart BUT did not complete purchase\n",
        "cart_users = df_raw.filter(F.col(\"event_type\") == \"ADD_TO_CART\").select(\"user_id\", \"product_name\").distinct()\n",
        "buyers = df_raw.filter(F.col(\"event_type\") == \"PURCHASE_COMPLETE\").select(\"user_id\").distinct()\n",
        "\n",
        "# The Anti-Join: \"Give me users in Cart bucket who are NOT in Buyers bucket\"\n",
        "# This is a classic Big Data pattern for retargeting\n",
        "df_abandoned = cart_users.join(buyers, on=\"user_id\", how=\"left_anti\")\n",
        "\n",
        "print(\"‚úÖ Analysis Complete.\")\n",
        "print(f\"   - Payment Errors Detected: {failure_count}\")\n",
        "print(f\"   - Critical Failure Rate: {fail_rate}% (Anomaly Detected!)\")\n",
        "print(f\"   - Users with Abandoned Carts (Retargeting List): {df_abandoned.count()}\")\n",
        "\n",
        "print(\"\\nüîé Top 5 Abandoned Products (Potential Pricing Issue?):\")\n",
        "df_abandoned.groupBy(\"product_name\").count().orderBy(\"count\", ascending=False).show(5, truncate=False)"
      ],
      "metadata": {
        "id": "D3td_2Sc-7Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **üîç What just happened?**\n",
        "\n",
        "> We utilized **Apache Spark** to perform distributed transformations on the log data.\n",
        "> * **Anomaly Detection:** We calculated a failure rate at the payment gateway (simulated around ~70%), identifying a critical bottleneck in the sales funnel.\n",
        "> * **Anti-Join Pattern:** We used a `left_anti` join to identify \"High Intent\" users‚Äîthose who added items to their cart but never appeared in the `PURCHASE_COMPLETE` list. This dataset is typically sent to marketing tools for retargeting campaigns."
      ],
      "metadata": {
        "id": "5nPkhn-I_HYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Step 4: Visualize Insights (Databricks Simulation)\n",
        " Visualizing the Funnel Drop-off and Error sources."
      ],
      "metadata": {
        "id": "_Cnxt4DE_Nn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üìä Step 4: Visualize Insights (Databricks Simulation)\n",
        "# Visualizing the Funnel Drop-off and Error sources.\n",
        "\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "# Convert Spark DF to Pandas for Plotting (Standard practice for aggregated data)\n",
        "pdf_funnel = df_funnel.toPandas()\n",
        "\n",
        "# Define Logic Order for Funnel\n",
        "funnel_order = ['VIEW_PRODUCT', 'ADD_TO_CART', 'CHECKOUT_START', 'PURCHASE_COMPLETE']\n",
        "pdf_funnel = pdf_funnel[pdf_funnel['event_type'].isin(funnel_order)]\n",
        "\n",
        "# Sort by custom order\n",
        "pdf_funnel['event_type'] = pd.Categorical(pdf_funnel['event_type'], categories=funnel_order, ordered=True)\n",
        "pdf_funnel = pdf_funnel.sort_values('event_type')\n",
        "\n",
        "print(\"üìä Rendering Funnel Visualization...\")\n",
        "\n",
        "# Chart 1: The Funnel\n",
        "# FIXED: Changed color sequence from 'Deep' to 'Viridis' (Universally supported)\n",
        "fig = px.funnel(\n",
        "    pdf_funnel,\n",
        "    x='count',\n",
        "    y='event_type',\n",
        "    title='üìâ E-Commerce Conversion Funnel (User Drop-off)',\n",
        "    labels={'event_type': 'Stage', 'count': 'Events'},\n",
        "    color='count',\n",
        "    color_discrete_sequence=px.colors.sequential.Viridis\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "# Chart 2: Error Analysis\n",
        "# Show Error Distribution by Device\n",
        "pdf_errors = df_raw.filter(F.col(\"event_type\") == \"ERROR_PAYMENT\").groupBy(\"device\").count().toPandas()\n",
        "\n",
        "fig2 = px.pie(\n",
        "    pdf_errors, values='count', names='device',\n",
        "    title='‚ö†Ô∏è Payment Failures by Device (Technical Debugging)',\n",
        "    hole=0.4,\n",
        "    color_discrete_sequence=px.colors.qualitative.Pastel # Nice soft colors for pie charts\n",
        ")\n",
        "fig2.show()"
      ],
      "metadata": {
        "id": "bh5v4f77_N_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **üîç What just happened?**\n",
        "\n",
        "> We visualized the processed data to communicate findings to stakeholders.\n",
        "> * **Funnel Chart:** Clearly visualizes the drop-off at each stage of the user journey, highlighting the massive gap between \"Checkout Start\" and \"Purchase Complete\".\n",
        "> * **Device Breakdown:** Helps engineers isolate if the payment error is specific to a platform (e.g., iOS App) or a general backend failure."
      ],
      "metadata": {
        "id": "gDvdeq7h_wYL"
      }
    }
  ]
}