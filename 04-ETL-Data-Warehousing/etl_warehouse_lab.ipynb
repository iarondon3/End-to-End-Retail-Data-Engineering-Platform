{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrMAEoFoa3OgCWCXJmHxwx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iarondon3/End-to-End-Retail-Data-Ecosystem/blob/main/04-ETL-Data-Warehousing/etl_warehouse_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âš™ï¸ Step 1: Install ETL & OLAP Tools (DuckDB)"
      ],
      "metadata": {
        "id": "mAMIwccqgbzC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIj1-FX0gU-B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "print(\"ðŸ“¦ Installing DuckDB, Faker & Analytics Libraries...\")\n",
        "\n",
        "# DuckDB is an in-process SQL OLAP database. Perfect for Colab.\n",
        "# CORRECTION: Added 'faker' to the installation list\n",
        "os.system(\"pip install duckdb pandas plotly faker > /dev/null 2>&1\")\n",
        "\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "elapsed = round(time.time() - start_time, 2)\n",
        "print(f\"âœ… Setup Completed in {elapsed}s. Ready to build the Data Warehouse!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **ðŸ” What just happened?**\n",
        "\n",
        "> We set up a **Modern Data Stack** environment within Colab.\n",
        "> * **DuckDB:** Instead of a heavy server, we use DuckDB, an in-process SQL OLAP database that allows us to run high-performance analytical queries directly on local data.\n",
        "> * **Libraries:** We installed `Plotly` to simulate the interactive dashboards usually found in BI tools like Metabase or Power BI."
      ],
      "metadata": {
        "id": "HwWoHKg8u_z2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ—ï¸ Step 2: Extract - Generate Raw Operational Data (OLTP)\n",
        "\n",
        "We simulate a \"messy\" source system to demonstrate the need for cleaning.\n",
        "\n",
        "Enter the volume of data to reconstruct the environment and click play"
      ],
      "metadata": {
        "id": "5TVNkXFHgm-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown **Data Volume:**\n",
        "TRANSACTIONS = 50000 # @param {type:\"slider\", min:5000, max:50000, step:5000}\n",
        "\n",
        "fake = Faker()\n",
        "print(f\"ðŸŽ² Generating {TRANSACTIONS} raw transactions...\")\n",
        "\n",
        "# --- A. WALGREENS CATALOG ---\n",
        "CATEGORIES_LIST = [\"Prescription Drugs (Rx)\", \"Over-the-Counter (OTC)\", \"Pain Relief\", \"Vitamins & Supplements\", \"Skin Care\", \"Personal Care\", \"Household\", \"Snacks\", \"Beverages\"]\n",
        "\n",
        "PRODUCT_TEMPLATES = {\n",
        "    \"Prescription Drugs (Rx)\": [\"Metformin 500mg\", \"Lisinopril 10mg\", \"Atorvastatin 20mg\"],\n",
        "    \"Over-the-Counter (OTC)\": [\"Allegra 24hr\", \"Zyrtec 10mg\", \"Claritin 24hr\"],\n",
        "    \"Pain Relief\": [\"Ibuprofen 200mg\", \"Advil Liqui-Gels\", \"Tylenol Extra Strength\"],\n",
        "    \"Vitamins & Supplements\": [\"Vitamin C 1000mg\", \"Vitamin D3 2000IU\", \"Omega-3 Fish Oil\"],\n",
        "    \"Skin Care\": [\"CeraVe Moisturizing Cream\", \"Neutrogena Hydro Boost\", \"Vaseline Jelly\"],\n",
        "    \"Personal Care\": [\"Head & Shoulders Shampoo\", \"Dove Deodorant\", \"Crest 3D White\"],\n",
        "    \"Household\": [\"Clorox Wipes\", \"Tide Detergent\", \"Charmin Ultra Soft\"],\n",
        "    \"Snacks\": [\"Lay's Classic Chips\", \"Doritos Nacho Cheese\", \"Oreo Cookies\"],\n",
        "    \"Beverages\": [\"Coca-Cola 2L\", \"Pepsi 2L\", \"Gatorade Fruit Punch\"]\n",
        "}\n",
        "\n",
        "# Generate Master Products\n",
        "product_master = []\n",
        "prod_id_counter = 1\n",
        "for cat in CATEGORIES_LIST:\n",
        "    items = PRODUCT_TEMPLATES.get(cat, [\"Generic Item\"])\n",
        "    for name in items:\n",
        "        base_cost = round(random.uniform(2.0, 50.0), 2)\n",
        "        product_master.append({\"id\": prod_id_counter, \"name\": name, \"category\": cat, \"unit_cost\": base_cost})\n",
        "        prod_id_counter += 1\n",
        "\n",
        "# --- B. BRANCHES (Fixed: 1 Branch per State = 10 Branches) ---\n",
        "STATES_LIST = ['FL', 'TX', 'CA', 'IL', 'NY', 'PA', 'NC', 'GA', 'OH', 'MI']\n",
        "branch_master = []\n",
        "\n",
        "# Create exactly 10 branches, mapping ID 1 -> FL, ID 2 -> TX, etc.\n",
        "for i, state in enumerate(STATES_LIST, 1):\n",
        "    branch_master.append({\n",
        "        \"id\": i,\n",
        "        \"state\": state\n",
        "    })\n",
        "\n",
        "# --- C. GENERATE TRANSACTIONS ---\n",
        "raw_data = []\n",
        "\n",
        "for _ in range(TRANSACTIONS):\n",
        "    prod = random.choice(product_master)\n",
        "    branch = random.choice(branch_master) # Pick one of the 10 stores\n",
        "\n",
        "    markup = random.uniform(1.2, 1.6)\n",
        "    sell_price = round(prod['unit_cost'] * markup, 2)\n",
        "\n",
        "    raw_data.append({\n",
        "        'transaction_id': fake.uuid4(),\n",
        "        'date_str': fake.date_between(start_date='-1y', end_date='today').strftime(\"%Y-%m-%d\"),\n",
        "        'product_id': prod['id'],\n",
        "        'product_name': prod['name'],\n",
        "        'category': prod['category'],\n",
        "        'branch_id': branch['id'],      # IDs will be 1 to 10\n",
        "        'branch_state': branch['state'],\n",
        "        'qty': random.randint(1, 4),\n",
        "        'unit_price': sell_price,\n",
        "        'unit_cost': prod['unit_cost']\n",
        "    })\n",
        "\n",
        "df_staging = pd.DataFrame(raw_data)\n",
        "print(f\"âœ… Extraction Complete. Staging Data Shape: {df_staging.shape}\")\n",
        "print(f\"   Unique Branches Generated: {df_staging['branch_id'].nunique()} (Should be 10)\")\n",
        "df_staging.head(3)"
      ],
      "metadata": {
        "id": "NECR57wegncl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **ðŸ” What just happened?**\n",
        "\n",
        "> We simulated the extraction from a **Legacy OLTP System**.\n",
        "> * **Staging Area:** We generated raw, unpolished transactional data. Notice that the data is \"dirty\" (dates are strings, costs are hidden) and flat.\n",
        "> * **The Challenge:** In this raw format, answering questions like *\"Which state is most profitable?\"* is computationally expensive. This justifies the need for the ETL process."
      ],
      "metadata": {
        "id": "2iERXwjQvJW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  ðŸ”„ Step 3: Transform - Business Logic & Star Schema Design\n",
        "\n",
        "Modeling the Star Schema: Fact_Sales + Dimensions (Product, Customer, Branch, Date)"
      ],
      "metadata": {
        "id": "6FrP0PEHi0hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ðŸ› ï¸ Starting ETL Transformation Pipeline...\")\n",
        "\n",
        "# --- 1. DATA ENRICHMENT ---\n",
        "\n",
        "# A. Generate Customer Data\n",
        "num_rows = len(df_staging)\n",
        "# Sequential IDs for Customers\n",
        "df_staging['customer_id'] = [random.randint(1, 1000) for _ in range(num_rows)]\n",
        "df_staging['customer_first'] = [fake.first_name() for _ in range(num_rows)]\n",
        "df_staging['customer_last'] = [fake.last_name() for _ in range(num_rows)]\n",
        "\n",
        "# B. Calculate Financial Metrics\n",
        "df_staging['date'] = pd.to_datetime(df_staging['date_str'])\n",
        "df_staging['total_revenue'] = df_staging['qty'] * df_staging['unit_price']\n",
        "df_staging['total_cost'] = df_staging['qty'] * df_staging['unit_cost']\n",
        "df_staging['gross_margin'] = df_staging['total_revenue'] - df_staging['total_cost']\n",
        "df_staging['margin_percentage'] = round((df_staging['gross_margin'] / df_staging['total_revenue']) * 100, 1)\n",
        "\n",
        "# --- 2. DIMENSIONAL MODELING ---\n",
        "\n",
        "# ðŸŒŸ DIMENSION 1: PRODUCT\n",
        "dim_product = df_staging[['product_id', 'product_name', 'category']].drop_duplicates(subset=['product_id'])\n",
        "dim_product = dim_product.rename(columns={'product_name': 'name', 'category': 'category_name'})\n",
        "dim_product = dim_product.sort_values(by='product_id').reset_index(drop=True)\n",
        "\n",
        "# ðŸŒŸ DIMENSION 2: CUSTOMER\n",
        "dim_customer = df_staging[['customer_id', 'customer_first', 'customer_last']].drop_duplicates(subset=['customer_id'])\n",
        "dim_customer['full_name'] = dim_customer['customer_first'] + \" \" + dim_customer['customer_last']\n",
        "dim_customer = dim_customer[['customer_id', 'full_name']]\n",
        "dim_customer = dim_customer.sort_values(by='customer_id').reset_index(drop=True)\n",
        "\n",
        "# ðŸŒŸ DIMENSION 3: BRANCH\n",
        "dim_branch = df_staging[['branch_id', 'branch_state']].drop_duplicates(subset=['branch_id'])\n",
        "dim_branch = dim_branch.rename(columns={'branch_state': 'state'})\n",
        "# FORCE SORT to ensure 1, 2, 3... order in the final table\n",
        "dim_branch = dim_branch.sort_values(by='branch_id').reset_index(drop=True)\n",
        "\n",
        "# ðŸŒŸ DIMENSION 4: DATE (SURROGATE KEY)\n",
        "unique_dates = df_staging[['date']].drop_duplicates().sort_values(by='date').reset_index(drop=True)\n",
        "unique_dates['date_id'] = range(1, len(unique_dates) + 1)\n",
        "unique_dates['year'] = unique_dates['date'].dt.year\n",
        "unique_dates['quarter'] = unique_dates['date'].dt.quarter\n",
        "unique_dates['month'] = unique_dates['date'].dt.month\n",
        "unique_dates['month_name'] = unique_dates['date'].dt.month_name()\n",
        "unique_dates['day_name'] = unique_dates['date'].dt.day_name()\n",
        "dim_date = unique_dates[['date_id', 'date', 'year', 'quarter', 'month', 'month_name', 'day_name']]\n",
        "\n",
        "# ðŸŒŸ FACT TABLE PREP (MERGE)\n",
        "df_fact_merged = pd.merge(df_staging, unique_dates[['date', 'date_id']], on='date', how='left')\n",
        "\n",
        "# ðŸŒŸ FACT TABLE: SALES\n",
        "fact_sales = df_fact_merged[[\n",
        "    'transaction_id',\n",
        "    'date_id',\n",
        "    'product_id',\n",
        "    'customer_id',\n",
        "    'branch_id',\n",
        "    'qty',\n",
        "    'unit_price',\n",
        "    'unit_cost',\n",
        "    'total_revenue',\n",
        "    'gross_margin',\n",
        "    'margin_percentage'\n",
        "]]\n",
        "\n",
        "print(\"âœ… Transformation Complete! Star Schema Constructed.\")\n",
        "\n",
        "# --- 3. FINAL SCHEMA AUDIT ---\n",
        "print(\"\\nðŸ”Ž FINAL SCHEMA AUDIT (Headers + 3 Rows Sample):\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "dataframes = {\n",
        "    \"ðŸ“¦ Dim_Product\": dim_product,\n",
        "    \"ðŸ“ Dim_Branch\": dim_branch,\n",
        "    \"ðŸ‘¥ Dim_Customer\": dim_customer,\n",
        "    \"ðŸ“… Dim_Date\": dim_date,\n",
        "    \"ðŸ’° Fact_Sales\": fact_sales\n",
        "}\n",
        "\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"\\n{name}  [Shape: {df.shape}]\")\n",
        "    print(\"-\" * 50)\n",
        "    # Showing 3 rows to confirm sequence 1, 2, 3\n",
        "    print(df.head(3).to_string(index=False))"
      ],
      "metadata": {
        "id": "tbI5F_Qvi1Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **ðŸ” What just happened?**\n",
        "\n",
        "> We executed the **Transformation Engine** (Python/Pandas).\n",
        "> * **Dimensional Modeling:** We decomposed the flat data into a **Star Schema**, creating distinct Dimensions (`Product`, `Branch`, `Customer`, `Date`) and a central Fact Table `Sales`.\n",
        "> * **Business Logic:** We applied financial rules to calculate **Gross Margin** and **Profitability %**, enriching the data before it hits the warehouse.\n",
        "> * **Data Quality:** We enforced sorted Surrogate Keys (IDs 1, 2, 3...) to ensure clean and indexable relationships."
      ],
      "metadata": {
        "id": "cysyx-CjvXiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ’¾ Step 4: Load - Building the Star Schema in DuckDB\n",
        "\n",
        " Loading the transformed DataFrames into the OLAP Engine."
      ],
      "metadata": {
        "id": "giBo0G82tzmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ðŸ—ï¸ Initializing Data Warehouse (DuckDB)...\")\n",
        "# We connect to an in-memory database.\n",
        "# In a real scenario, this would be a connection string to AWS Redshift or Snowflake.\n",
        "con = duckdb.connect(database=':memory:')\n",
        "\n",
        "# --- THE LOAD PROCESS (Python -> SQL) ---\n",
        "# This is where the DataFrames become real SQL Tables\n",
        "print(\"   Loading Dimensions...\")\n",
        "con.execute(\"CREATE TABLE Dim_Product AS SELECT * FROM dim_product\")\n",
        "con.execute(\"CREATE TABLE Dim_Customer AS SELECT * FROM dim_customer\")\n",
        "con.execute(\"CREATE TABLE Dim_Branch AS SELECT * FROM dim_branch\")\n",
        "con.execute(\"CREATE TABLE Dim_Date AS SELECT * FROM dim_date\")\n",
        "\n",
        "print(\"   Loading Fact Table...\")\n",
        "con.execute(\"CREATE TABLE Fact_Sales AS SELECT * FROM fact_sales\")\n",
        "\n",
        "# --- POST-LOAD VERIFICATION ---\n",
        "print(\"\\nâœ… Load Complete. The Star Schema is ready inside DuckDB!\")\n",
        "\n",
        "print(\"\\nðŸ”Ž DATABASE CHECK (Running SQL 'SHOW TABLES'):\")\n",
        "# This query proves the tables exist in the DB engine\n",
        "tables = con.execute(\"SHOW TABLES\").df()\n",
        "print(tables.to_string(index=False))\n",
        "\n",
        "print(\"\\nðŸ”Ž INTEGRITY CHECK (Testing a SQL Join):\")\n",
        "# Let's prove the tables are connected. We join Fact to Branch to see if it works.\n",
        "sql_test = \"\"\"\n",
        "    SELECT\n",
        "        b.state,\n",
        "        COUNT(*) as total_sales,\n",
        "        SUM(f.total_revenue) as revenue\n",
        "    FROM Fact_Sales f\n",
        "    JOIN Dim_Branch b ON f.branch_id = b.branch_id\n",
        "    GROUP BY b.state\n",
        "    ORDER BY revenue DESC\n",
        "    LIMIT 3\n",
        "\"\"\"\n",
        "print(\"   Executing analytical query to verify relationships...\")\n",
        "print(con.execute(sql_test).df().to_string(index=False))"
      ],
      "metadata": {
        "id": "Eemq2OaUtz8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **ðŸ” What just happened?**\n",
        "\n",
        "> We performed the **Load (L)** phase, persisting data into the Data Warehouse.\n",
        "> * **From Memory to SQL:** We moved the Python DataFrames into **DuckDB Tables**. Now the data is no longer just variables in RAM, but structured SQL tables.\n",
        "> * **Integrity Check:** The final SQL query proved that our Star Schema works: we successfully joined `Fact_Sales` with `Dim_Branch` to aggregate revenue by State, confirming the relationships are valid."
      ],
      "metadata": {
        "id": "L00ULT3QvgTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“Š Step 5: Automated BI Reporting (Data Storytelling)\n",
        "\n",
        "\n",
        "Strategy: Instead of manual widgets, we automate the generation of insights.\n",
        "\n",
        " 1. Generate National Report.\n",
        " 2. Detect Top Performing State via SQL.\n",
        " 3. Generate Regional Deep-Dive automatically."
      ],
      "metadata": {
        "id": "UPhM3JuMwhD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "\n",
        "# Force Colab Rendering\n",
        "pio.renderers.default = 'colab'\n",
        "\n",
        "def generate_dashboard(filter_value):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    if filter_value == 'All USA':\n",
        "        print(f\"ðŸŒŽ GENERATING REPORT: NATIONAL OVERVIEW\")\n",
        "        where_clause = \"\"\n",
        "        context_sql = \"\"\"\n",
        "            SELECT b.state, SUM(f.total_revenue) as Metric\n",
        "            FROM Fact_Sales f\n",
        "            JOIN Dim_Branch b ON f.branch_id = b.branch_id\n",
        "            GROUP BY b.state ORDER BY Metric DESC\n",
        "        \"\"\"\n",
        "        chart_title = \"ðŸ“ Revenue by State\"\n",
        "        col_name = 'state'\n",
        "        color_scale = 'Blues'\n",
        "    else:\n",
        "        print(f\"â­ GENERATING DEEP-DIVE: {filter_value}\")\n",
        "        where_clause = f\"WHERE b.state = '{filter_value}'\"\n",
        "        context_sql = f\"\"\"\n",
        "            SELECT p.name as Product, SUM(f.total_revenue) as Metric\n",
        "            FROM Fact_Sales f\n",
        "            JOIN Dim_Product p ON f.product_id = p.product_id\n",
        "            JOIN Dim_Branch b ON f.branch_id = b.branch_id\n",
        "            {where_clause}\n",
        "            GROUP BY p.name ORDER BY Metric DESC LIMIT 5\n",
        "        \"\"\"\n",
        "        chart_title = f\"ðŸ† Top 5 Best Sellers in {filter_value}\"\n",
        "        col_name = 'Product'\n",
        "        color_scale = 'Oranges'\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # --- EXECUTE QUERIES ---\n",
        "    # 1. KPI Category\n",
        "    df_cat = con.execute(f\"\"\"\n",
        "        SELECT p.category_name, SUM(f.total_revenue) as Revenue, SUM(f.gross_margin) as Margin\n",
        "        FROM Fact_Sales f\n",
        "        JOIN Dim_Product p ON f.product_id = p.product_id\n",
        "        JOIN Dim_Branch b ON f.branch_id = b.branch_id\n",
        "        {where_clause}\n",
        "        GROUP BY p.category_name ORDER BY Revenue DESC\n",
        "    \"\"\").df()\n",
        "\n",
        "    # 2. KPI Trend\n",
        "    df_trend = con.execute(f\"\"\"\n",
        "        SELECT d.year, d.month, d.month_name, SUM(f.total_revenue) as Revenue, SUM(f.gross_margin) as Margin\n",
        "        FROM Fact_Sales f\n",
        "        JOIN Dim_Date d ON f.date_id = d.date_id\n",
        "        JOIN Dim_Branch b ON f.branch_id = b.branch_id\n",
        "        {where_clause}\n",
        "        GROUP BY d.year, d.month, d.month_name ORDER BY d.year, d.month\n",
        "    \"\"\").df()\n",
        "    df_trend['Period'] = df_trend['month_name'] + \" \" + df_trend['year'].astype(str)\n",
        "\n",
        "    # 3. Context Data\n",
        "    df_geo = con.execute(context_sql).df()\n",
        "\n",
        "    # --- RENDER CHARTS ---\n",
        "    fig1 = px.bar(\n",
        "        df_cat, x='category_name', y=['Revenue', 'Margin'],\n",
        "        title=f\"ðŸ’° Profitability by Category ({filter_value})\",\n",
        "        barmode='group', height=300,\n",
        "        color_discrete_map={'Revenue': '#336791', 'Margin': '#47A248'}\n",
        "    )\n",
        "    fig1.show()\n",
        "\n",
        "    fig2 = px.line(\n",
        "        df_trend, x='Period', y=['Revenue', 'Margin'],\n",
        "        title=f\"ðŸ“ˆ Sales Trend ({filter_value})\",\n",
        "        markers=True, height=300\n",
        "    )\n",
        "    fig2.show()\n",
        "\n",
        "    fig3 = px.bar(\n",
        "        df_geo, x=col_name, y='Metric',\n",
        "        title=chart_title, text_auto='.2s', height=300,\n",
        "        color='Metric', color_continuous_scale=color_scale\n",
        "    )\n",
        "    fig3.show()\n",
        "\n",
        "# --- RUN AUTOMATED STORY ---\n",
        "\n",
        "# 1. Show National Data\n",
        "generate_dashboard('All USA')\n",
        "\n",
        "# 2. Find the \"Best\" State automatically via SQL\n",
        "print(\"\\nðŸ¤– AI ANALYST: Detecting Top Performing State...\")\n",
        "top_state_query = \"\"\"\n",
        "    SELECT b.state\n",
        "    FROM Fact_Sales f\n",
        "    JOIN Dim_Branch b ON f.branch_id = b.branch_id\n",
        "    GROUP BY b.state\n",
        "    ORDER BY SUM(f.total_revenue) DESC\n",
        "    LIMIT 1\n",
        "\"\"\"\n",
        "best_state = con.execute(top_state_query).fetchone()[0]\n",
        "print(f\"âœ… Detection Complete. Top Market is: {best_state}\")\n",
        "\n",
        "# 3. Show Top State Data\n",
        "generate_dashboard(best_state)"
      ],
      "metadata": {
        "id": "SPRRzpwnwhS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **ðŸ” What just happened?**\n",
        "\n",
        "> We executed an **Automated Data Storytelling** script.\n",
        "> * **Batch Reporting:** Instead of waiting for user clicks, the system automatically generated a **National Overview**.\n",
        "> * **SQL Logic Injection:** The script then queried the Data Warehouse to identify the **Top Performing State** dynamically (detecting the winner based on current data).\n",
        "> * **Contextual Deep-Dive:** Upon detection, it automatically re-ran the dashboard filters to generate a specific report for that state, showing its top-selling products. This simulates a nightly automated email report sent to regional managers."
      ],
      "metadata": {
        "id": "wkwk7Z0ovqKu"
      }
    }
  ]
}